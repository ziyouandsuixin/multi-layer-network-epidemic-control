# experiments/evaluation/layered_evaluator.py
import os
import yaml
import numpy as np
from typing import Dict, Any, List
from .basic_evaluator import BasicCapabilityEvaluator, create_basic_evaluator
from .intelligent_evaluator import IntelligentCapabilityEvaluator, create_intelligent_evaluator

class LayeredEvaluator:
    """åˆ†å±‚è¯„ä¼°å™¨ - æ•´åˆåŸºç¡€èƒ½åŠ›å’Œæ™ºèƒ½èƒ½åŠ›"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.basic_evaluator = create_basic_evaluator(config_path)
        self.intelligent_evaluator = create_intelligent_evaluator(config_path)
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½è¯„ä¼°é…ç½®"""
        default_config = {
            'layer_weights': {
                'basic': 0.40,
                'intelligent': 0.60
            },
            'ranking_thresholds': {
                'excellent': 0.8,
                'good': 0.6,
                'fair': 0.4,
                'poor': 0.0
            }
        }
        return default_config
    
    def evaluate_single_model(self, model_name: str, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n{'='*50}")
        print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
        print(f"{'='*50}")
        
        evaluation = {'model_name': model_name}
        
        try:
            # åŸºç¡€èƒ½åŠ›è¯„ä¼°
            print("ğŸ“Š è¿›è¡ŒåŸºç¡€èƒ½åŠ›è¯„ä¼°...")
            basic_scores = self.basic_evaluator.evaluate(model_results)
            evaluation['basic_capability'] = basic_scores
            
            # æ™ºèƒ½èƒ½åŠ›è¯„ä¼°
            print("è¿›è¡Œæ™ºèƒ½èƒ½åŠ›è¯„ä¼°...")
            intelligent_scores = self.intelligent_evaluator.evaluate(model_name, model_results)
            evaluation['intelligent_capability'] = intelligent_scores
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            final_score = self._calculate_final_score(basic_scores, intelligent_scores)
            evaluation['final_score'] = final_score
            
            # æ€§èƒ½ç­‰çº§
            evaluation['performance_level'] = self._get_performance_level(final_score)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            evaluation['analysis'] = self._generate_analysis(evaluation)
            
            print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ: {final_score:.3f} ({evaluation['performance_level']})")
            
        except Exception as e:
            print(f"{model_name} è¯„ä¼°å¤±è´¥: {e}")
            evaluation = self._create_fallback_evaluation(model_name)
            
        return evaluation
    
    def evaluate_all_models(self, all_results: Dict[str, Dict]) -> Dict[str, Any]:
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹å¹¶æ’å"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(all_results)} ä¸ªæ¨¡å‹")
        print(f"{'='*60}")
        
        evaluations = {}
        
        for model_name, results in all_results.items():
            evaluation = self.evaluate_single_model(model_name, results)
            evaluations[model_name] = evaluation
        
        # ç”Ÿæˆæ’å
        ranked_models = self._rank_models(evaluations)
        
        final_report = {
            'evaluations': evaluations,
            'ranking': ranked_models,
            'summary': self._generate_summary(ranked_models)
        }
        
        self._print_comprehensive_report(final_report)
        
        return final_report
    
    def perform_weight_sensitivity_analysis(self, evaluation_data, weight_combinations=None):
        """
        æƒé‡æ•æ„Ÿæ€§åˆ†æ - æµ‹è¯•ä¸åŒæƒé‡é…ç½®çš„æ’åç¨³å®šæ€§
        """
        if weight_combinations is None:
            weight_combinations = [
                (0.3, 0.7),  # å€¾å‘æ™ºèƒ½èƒ½åŠ›
                (0.4, 0.6),  # å½“å‰è®¾ç½®
                (0.5, 0.5),  # å¹³è¡¡è®¾ç½®
                (0.6, 0.4)   # å€¾å‘åŸºç¡€èƒ½åŠ›
            ]
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æƒé‡æ•æ„Ÿæ€§åˆ†æ")
        print(f"{'='*60}")
        
        sensitivity_results = {}
        original_weights = self.config['layer_weights'].copy()
        
        for i, (basic_w, intelligent_w) in enumerate(weight_combinations):
            print(f"\næµ‹è¯•æƒé‡ç»„åˆ {i+1}/{len(weight_combinations)}: {basic_w}:{intelligent_w}")
            
            # ä¸´æ—¶ä¿®æ”¹æƒé‡
            self.config['layer_weights'] = {'basic': basic_w, 'intelligent': intelligent_w}
            
            # ä½¿ç”¨ç°æœ‰è¯„ä¼°é€»è¾‘
            evaluation_report = self.evaluate_all_models(evaluation_data)
            
            # è®°å½•æ’å
            ranking = [item['model'] for item in evaluation_report['ranking']]
            sensitivity_results[f"{basic_w}:{intelligent_w}"] = {
                'ranking': ranking,
                'best_model': ranking[0],
                'scores': {item['model']: item['score'] for item in evaluation_report['ranking']}
            }
        
        # æ¢å¤åŸå§‹æƒé‡
        self.config['layer_weights'] = original_weights
        
        # åˆ†ææ•æ„Ÿæ€§
        self._analyze_sensitivity_results(sensitivity_results)
        
        return sensitivity_results
    
    def _analyze_sensitivity_results(self, sensitivity_results):
        """åˆ†ææƒé‡æ•æ„Ÿæ€§ç»“æœ"""
        print(f"\n{'='*70}")
        print(f"æƒé‡æ•æ„Ÿæ€§åˆ†æç»“æœ")
        print(f"{'='*70}")
        
        # æ£€æŸ¥æœ€ä½³æ¨¡å‹çš„ç¨³å®šæ€§
        best_models = [result['best_model'] for result in sensitivity_results.values()]
        unique_best_models = set(best_models)
        
        print(f"\næœ€ä½³æ¨¡å‹ç¨³å®šæ€§åˆ†æ:")
        for weight, result in sensitivity_results.items():
            print(f"  æƒé‡ {weight}: æœ€ä½³æ¨¡å‹ = {result['best_model']}")
        
        if len(unique_best_models) == 1:
            print(f"\nä¼˜ç§€: æœ€ä½³æ¨¡å‹åœ¨æ‰€æœ‰æƒé‡é…ç½®ä¸‹ä¿æŒä¸€è‡´")
            print(f"   ç¨³å®šæœ€ä½³æ¨¡å‹: {list(unique_best_models)[0]}")
        else:
            print(f"\næ³¨æ„: æœ€ä½³æ¨¡å‹éšæƒé‡å˜åŒ–")
            print(f"   å‡ºç°çš„æ‰€æœ‰æœ€ä½³æ¨¡å‹: {', '.join(unique_best_models)}")
        
        # æ’åä¸€è‡´æ€§åˆ†æ
        print(f"\næ’åä¸€è‡´æ€§åˆ†æ:")
        all_models = set()
        for result in sensitivity_results.values():
            all_models.update(result['ranking'])
        
        ranking_consistency = {}
        for model in all_models:
            ranks = []
            for weight, result in sensitivity_results.items():
                if model in result['ranking']:
                    ranks.append(result['ranking'].index(model) + 1)
            ranking_consistency[model] = {
                'mean_rank': np.mean(ranks),
                'std_rank': np.std(ranks),
                'min_rank': min(ranks),
                'max_rank': max(ranks)
            }
        
        for model, consistency in sorted(ranking_consistency.items(), key=lambda x: x[1]['mean_rank']):
            stability = "é«˜" if consistency['std_rank'] < 1.0 else "ä¸­" if consistency['std_rank'] < 2.0 else "ä½"
            print(f"  {model:<25}: å¹³å‡æ’å {consistency['mean_rank']:.1f} Â± {consistency['std_rank']:.1f} "
                  f"(èŒƒå›´: {consistency['min_rank']}-{consistency['max_rank']}) - ç¨³å®šæ€§: {stability}")
    
    def _calculate_final_score(self, basic_scores: Dict, intelligent_scores: Dict) -> float:
        """è®¡ç®—æœ€ç»ˆå¾—åˆ†"""
        basic_weight = self.config['layer_weights']['basic']
        intelligent_weight = self.config['layer_weights']['intelligent']
        
        basic_total = basic_scores.get('total_basic_score', 0)
        intelligent_total = intelligent_scores.get('total_intelligent_score', 0)
        
        return (basic_total * basic_weight + intelligent_total * intelligent_weight)
    
    def _get_performance_level(self, score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        thresholds = self.config['ranking_thresholds']
        
        if score >= thresholds['excellent']:
            return 'ä¼˜ç§€'
        elif score >= thresholds['good']:
            return 'è‰¯å¥½'
        elif score >= thresholds['fair']:
            return 'ä¸€èˆ¬'
        else:
            return 'å¾…æ”¹è¿›'
    
    def _generate_analysis(self, evaluation: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        model_name = evaluation['model_name']
        basic_scores = evaluation['basic_capability']
        intelligent_scores = evaluation['intelligent_capability']
        final_score = evaluation['final_score']
        
        analysis = {
            'summary': f"{model_name} ç»¼åˆå¾—åˆ†: {final_score:.3f}",
            'strengths': [],
            'improvements': [],
            'recommendations': []
        }
        
        # åˆ†æä¼˜åŠ¿
        if basic_scores.get('epidemic_control', 0) > 0.7:
            analysis['strengths'].append("ç–«æƒ…æ§åˆ¶æ•ˆæœæ˜¾è‘—")
        if basic_scores.get('resource_efficiency', 0) > 0.7:
            analysis['strengths'].append("èµ„æºåˆ©ç”¨é«˜æ•ˆ")
        if intelligent_scores.get('risk_prediction', 0) > 0.6:
            analysis['strengths'].append("å…·å¤‡é£é™©é¢„æµ‹èƒ½åŠ›")
        if intelligent_scores.get('targeted_intervention', 0) > 0.6:
            analysis['strengths'].append("æ”¯æŒç²¾å‡†å¹²é¢„")
        
        # åˆ†ææ”¹è¿›ç‚¹
        if basic_scores.get('epidemic_control', 0) < 0.5:
            analysis['improvements'].append("ç–«æƒ…æ§åˆ¶èƒ½åŠ›éœ€æå‡")
        if basic_scores.get('resource_efficiency', 0) < 0.5:
            analysis['improvements'].append("èµ„æºæ•ˆç‡æœ‰å¾…ä¼˜åŒ–")
        if intelligent_scores.get('total_intelligent_score', 0) < 0.4:
            analysis['improvements'].append("æ™ºèƒ½èƒ½åŠ›éœ€è¦åŠ å¼º")
        
        # ç”Ÿæˆå»ºè®®
        if not analysis['strengths']:
            analysis['recommendations'].append("éœ€è¦å…¨é¢æå‡æ¨¡å‹èƒ½åŠ›")
        elif analysis['improvements']:
            analysis['recommendations'].append(f"å»ºè®®é‡ç‚¹æ”¹è¿›: {', '.join(analysis['improvements'])}")
        else:
            analysis['recommendations'].append("æ¨¡å‹è¡¨ç°å‡è¡¡ï¼Œç»§ç»­ä¿æŒä¼˜ç§€è¡¨ç°")
        
        return analysis
    
    def _rank_models(self, evaluations: Dict[str, Dict]) -> List[Dict]:
        """å¯¹æ¨¡å‹è¿›è¡Œæ’å"""
        ranked = []
        
        for model_name, evaluation in evaluations.items():
            ranked.append({
                'model': model_name,
                'score': evaluation['final_score'],
                'level': evaluation['performance_level'],
                'basic_score': evaluation['basic_capability']['total_basic_score'],
                'intelligent_score': evaluation['intelligent_capability']['total_intelligent_score']
            })
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        ranked.sort(key=lambda x: x['score'], reverse=True)
        
        # æ·»åŠ æ’å
        for i, item in enumerate(ranked):
            item['rank'] = i + 1
        
        return ranked
    
    def _generate_summary(self, ranked_models: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if not ranked_models:
            return {}
            
        best_model = ranked_models[0]
        avg_score = sum(item['score'] for item in ranked_models) / len(ranked_models)
        
        return {
            'best_model': best_model['model'],
            'best_score': best_model['score'],
            'average_score': avg_score,
            'total_models': len(ranked_models),
            'excellent_count': sum(1 for item in ranked_models if item['level'] == 'ä¼˜ç§€'),
            'good_count': sum(1 for item in ranked_models if item['level'] == 'è‰¯å¥½')
        }
    
    def _print_comprehensive_report(self, final_report: Dict):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"æ¨¡å‹è¯„ä¼°ç»¼åˆæŠ¥å‘Š")
        print(f"{'='*70}")
        
        ranking = final_report['ranking']
        summary = final_report['summary']
        
        print(f"\nğŸ† æ¨¡å‹æ’å:")
        print(f"{'æ’å':<4} {'æ¨¡å‹':<25} {'ç»¼åˆå¾—åˆ†':<8} {'åŸºç¡€èƒ½åŠ›':<8} {'æ™ºèƒ½èƒ½åŠ›':<8} {'ç­‰çº§':<6}")
        print("-" * 70)
        
        for item in ranking:
            print(f"{item['rank']:<4} {item['model']:<25} {item['score']:<8.3f} "
                  f"{item['basic_score']:<8.3f} {item['intelligent_score']:<8.3f} {item['level']:<6}")
        
        print(f"\næ€»ç»“:")
        print(f"  â€¢ æœ€ä½³æ¨¡å‹: {summary['best_model']} (å¾—åˆ†: {summary['best_score']:.3f})")
        print(f"  â€¢ å¹³å‡å¾—åˆ†: {summary['average_score']:.3f}")
        print(f"  â€¢ ä¼˜ç§€æ¨¡å‹: {summary['excellent_count']} ä¸ª")
        print(f"  â€¢ è¯„ä¼°æ€»æ•°: {summary['total_models']} ä¸ª")
    
    def _create_fallback_evaluation(self, model_name: str) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€è¯„ä¼°ç»“æœ"""
        return {
            'model_name': model_name,
            'basic_capability': {'total_basic_score': 0.5},
            'intelligent_capability': {'total_intelligent_score': 0.35},
            'final_score': 0.44,
            'performance_level': 'ä¸€èˆ¬',
            'analysis': {
                'summary': f"{model_name} è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                'strengths': [],
                'improvements': ['è¯„ä¼°ç¨³å®šæ€§éœ€è¦æå‡'],
                'recommendations': ['æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸€è‡´æ€§']
            }
        }

# æ·»åŠ å¿«æ·å‡½æ•°
def create_layered_evaluator(config_path: str = None) -> LayeredEvaluator:
    """åˆ›å»ºåˆ†å±‚è¯„ä¼°å™¨å®ä¾‹"""
    return LayeredEvaluator(config_path)